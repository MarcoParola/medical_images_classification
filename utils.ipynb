{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcoParola/medical_images_classification/blob/main/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EScfUT3IO-Ta"
      },
      "source": [
        "# **Utils**\r\n",
        "This notebook contains some functions used in other notebooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjzi0wyoNpB4"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsJPM8GZT7Ky"
      },
      "source": [
        "def load_data(dir):\n",
        "  imagesTrain = np.load(os.path.join(dir + 'train_tensor.npy'))\n",
        "  labelsTrain = np.load(os.path.join(dir + 'train_labels.npy'))\n",
        "  imagesTestPublic = np.load(os.path.join(dir + 'public_test_tensor.npy'))\n",
        "  labelsTestPublic = np.load(os.path.join(dir + 'public_test_labels.npy'))\n",
        "  imagesTestPrivate = np.load(os.path.join(dir + 'private_test_tensor.npy'))\n",
        "\n",
        "  return imagesTrain, labelsTrain, imagesTestPublic, labelsTestPublic, imagesTestPrivate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNgA79TBIgQk"
      },
      "source": [
        "def scaleData(image):\n",
        "  scaledImage = image / (pow(2,16)-1)\n",
        "  return scaledImage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txJWasNmPbGH"
      },
      "source": [
        "## Utility function that plots the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6ZLc-BWoag8"
      },
      "source": [
        "def plot_confusionMatrix(model_, testSet_, testLabels_, classes):\r\n",
        "    pred = model_.predict_classes(testSet_)\r\n",
        "    cm = metrics.confusion_matrix(pred, testLabels_)\r\n",
        "    plt.imshow(cm, interpolation='nearest', cmap='OrRd')\r\n",
        "    plt.title('Confusion matrix')\r\n",
        "    plt.colorbar()\r\n",
        "    tick_marks = np.arange(len(classes))\r\n",
        "    print(tick_marks)\r\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\r\n",
        "    plt.yticks(tick_marks, classes)\r\n",
        "\r\n",
        "    cm = np.round( cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] ,2)\r\n",
        "    print(\"Normalized confusion matrix\")\r\n",
        "    thresh = 0.6\r\n",
        "    \r\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "        plt.text(j, i, cm[i, j],horizontalalignment=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.ylabel('True label')\r\n",
        "    plt.xlabel('Predicted label')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxzaHlEiodmT"
      },
      "source": [
        "## Utility function that plots roc curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWKe8MnA3W9k"
      },
      "source": [
        "def plotRocCurves(models, test, labels):\r\n",
        "  for i in range(len(models)):\r\n",
        "    probs = models[i].predict_proba(test)\r\n",
        "    preds = probs[:,1]\r\n",
        "    fpr, tpr, threshold = metrics.roc_curve(labels, preds)\r\n",
        "    roc_auc = metrics.auc(fpr, tpr)\r\n",
        "\r\n",
        "    # method I: plt\r\n",
        "    import matplotlib.pyplot as plt\r\n",
        "    plt.title('Roc curves')\r\n",
        "    plt.plot(fpr, tpr, 'b', color=(np.random.rand(),np.random.rand(),np.random.rand()), label = 'model' + str(i+1) + '= %0.2f' % roc_auc)\r\n",
        "    plt.legend(loc = 'lower right')\r\n",
        "    plt.plot([0, 1], [0, 1],'r--')\r\n",
        "    plt.xlim([0, 1])\r\n",
        "    plt.ylim([0, 1])\r\n",
        "    plt.ylabel('True Positive Rate')\r\n",
        "    plt.xlabel('False Positive Rate')\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa3ZjReV4lyL"
      },
      "source": [
        "def plotRocCurves(models, test, labels, names):\r\n",
        "  for i in range(len(models)):\r\n",
        "    probs = models[i].predict_proba(test)\r\n",
        "    preds = probs[:,1]\r\n",
        "    fpr, tpr, threshold = metrics.roc_curve(labels, preds)\r\n",
        "    roc_auc = metrics.auc(fpr, tpr)\r\n",
        "\r\n",
        "    # method I: plt\r\n",
        "    import matplotlib.pyplot as plt\r\n",
        "    plt.title('Roc curves')\r\n",
        "    plt.plot(fpr, tpr, 'b', color=(np.random.rand(),np.random.rand(),np.random.rand()), label = names[i] + '= %0.2f' % roc_auc)\r\n",
        "    plt.legend(loc = 'lower right')\r\n",
        "    plt.plot([0, 1], [0, 1],'r--')\r\n",
        "    plt.xlim([0, 1])\r\n",
        "    plt.ylim([0, 1])\r\n",
        "    plt.ylabel('True Positive Rate')\r\n",
        "    plt.xlabel('False Positive Rate')\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDe_CYrj3e5l"
      },
      "source": [
        "def plot_accurancy_loss(hist):\r\n",
        "  acc_1 = hist.history['accuracy']\r\n",
        "  val_acc_1 = hist.history['val_accuracy']\r\n",
        "  loss_1 = hist.history['loss']\r\n",
        "  val_loss_1 = hist.history['val_loss']\r\n",
        "\r\n",
        "  plt.ylim(0,1)\r\n",
        "  \r\n",
        "  epochs = range(len(acc_1))\r\n",
        "\r\n",
        "  plt.plot(epochs, acc_1, 'bo', label='Training acc')\r\n",
        "  plt.plot(epochs, val_acc_1, 'b', label='Validation acc')\r\n",
        "  plt.title('Training and validation accuracy')\r\n",
        "  plt.legend()\r\n",
        "\r\n",
        "  plt.figure()\r\n",
        "  plt.ylim(0,1)\r\n",
        "  plt.plot(epochs, loss_1, 'bo', label='Training loss')\r\n",
        "  plt.plot(epochs, val_loss_1, 'b', label='Validation loss')\r\n",
        "  plt.title('Training and validation loss')\r\n",
        "  plt.legend()\r\n",
        "\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}